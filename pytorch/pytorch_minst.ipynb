{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch MINST excercice following pythonprogramming.net tutorials\n",
    "\n",
    "https://pythonprogramming.net/building-deep-learning-neural-network-pytorch/?completed=/data-deep-learning-neural-network-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports relevant libraries \n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Downloads MINST dataset \n",
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divides the dataset between training data and test data\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Defines de network as a class (OOP)\n",
    "class Net(nn.Module):\n",
    "    # Method that sets the network shape\n",
    "    def __init__(self):\n",
    "        #super() runs __init__ in parent class\n",
    "        super().__init__()\n",
    "        # 28*28 is the number of pixels in image, 64 is number of neurons in first layer\n",
    "        self.fc1 = nn.Linear(28*28, 64) \n",
    "        # 64 is the number of neurons in first layer, 32 in second layer\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        # 32 is the number of neurons in second layer, 32 in third layer\n",
    "        self.fc3 = nn.Linear(32, 128)\n",
    "        # 128 is the number of neurons in third layer, 10 is the number of possible outcomes the network has (0-9)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "\n",
    "    #Method that defines the forward pass of the network\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random \"image\" of random pixels to pass through network\n",
    "X_random = torch.randn((28,28))\n",
    "#  Flattens the 28*28 matrix\n",
    "#  -1 tells pytorch that any batch of images can be passed, can be replaced for batch size\n",
    "X_random = X_random.view(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-31.5155,  -2.6058,  -1.0134,  -2.1185, -14.8103,  -0.8155, -13.4839,\n",
       "         -16.8941,  -7.4815, -14.5343]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Runs the random pixels through the network.  It Works!!\n",
    "output = net(X_random)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0043, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0076, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0070, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS): # 2 full passes over the data\n",
    "    for data in trainset:  # `data` is a batch of data\n",
    "        X, y = data  # X is the batch of features, y is the batch of targets.\n",
    "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
    "        output = net(X.view(-1,784))  # pass in the reshaped batch (recall they are 28x28 atm)\n",
    "        loss = F.nll_loss(output, y)  # calc and grab the loss value\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
    "    print(loss)  # print loss. We hope loss (a measure of wrong-ness) declines! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.973\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        #print(output)\n",
    "        for idx, i in enumerate(output):\n",
    "            #print(torch.argmax(i), y[idx])\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X[3].view(28,28))\n",
    "plt.show()\n",
    "\n",
    "print(torch.argmax(net(X[3].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
